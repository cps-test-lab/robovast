{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "845bbcf5",
   "metadata": {},
   "source": [
    "# ROS Bag Data Visualization - Folder with Single Runs\n",
    "\n",
    "This notebook provides comparative visualization for multiple single runs within a folder (e.g., room-generated-p1 with subfolders 0, 1, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8602c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "#csv_file_paths = glob.glob('../downloaded_files/run-2025-09-17-172852/rooms-generated-p1/.cache/**/*.csv', recursive=True)\n",
    "csv_file_paths = ['messages.csv']\n",
    "print(f'Loading data from {len(csv_file_paths)} csv files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine data from all runs\n",
    "all_dataframes = []\n",
    "run_names = []\n",
    "\n",
    "for csv_file in csv_file_paths: \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        run_name = Path(csv_file).stem\n",
    "        df['run_id'] = run_name\n",
    "        all_dataframes.append(df)\n",
    "        run_names.append(run_name)\n",
    "        print(f'Loaded {len(df)} rows from run: {run_name}')\n",
    "    except Exception as e:\n",
    "        print(f'Error loading {csv_file}: {e}')\n",
    "\n",
    "if all_dataframes:\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(f'\\nCombined dataset: {len(combined_df)} total rows from {len(run_names)} runs')\n",
    "else:\n",
    "    print('No data could be loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f549f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf35e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparative visualizations\n",
    "if all_dataframes and 'timestamp' in combined_df.columns:\n",
    "    numeric_cols = combined_df.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col != 'timestamp']\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        # Plot comparison of runs for each numeric column\n",
    "        fig, axes = plt.subplots(min(len(numeric_cols), 3), 1, figsize=(14, 4*min(len(numeric_cols), 3)))\n",
    "        if not isinstance(axes, np.ndarray):\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, col in enumerate(numeric_cols[:3]):\n",
    "            for run_name in run_names:\n",
    "                run_data = combined_df[combined_df['run_id'] == run_name]\n",
    "                if len(run_data) > 0 and col in run_data.columns:\n",
    "                    axes[i].plot(run_data['timestamp'], run_data[col], \n",
    "                               label=f'Run {run_name}', alpha=0.7, linewidth=1.5)\n",
    "            \n",
    "            axes[i].set_title(f'{col} - Comparison Across Runs')\n",
    "            axes[i].set_ylabel(col)\n",
    "            axes[i].legend()\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[-1].set_xlabel('Timestamp')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No numeric columns found for visualization')\n",
    "else:\n",
    "    print('No suitable data for comparative visualization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9671fd",
   "metadata": {},
   "source": [
    "# Real Time Factor Analysis\n",
    "\n",
    "Real-time factor analysis shows how well the simulation performs compared to real-time. A value of 1.0 means the simulation runs at exactly real-time speed, values < 1.0 indicate the simulation is slower than real-time, and values > 1.0 indicate faster than real-time execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1775161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Time Factor Analysis\n",
    "if all_dataframes and len(combined_df) > 0:\n",
    "    # Check if we have real_time_factor data\n",
    "    rtf_data = combined_df[combined_df['topic'] == '/gazebo/real_time_factor'] if 'topic' in combined_df.columns else combined_df\n",
    "    \n",
    "    if len(rtf_data) > 0:\n",
    "        print(f\"Found {len(rtf_data)} real_time_factor data points across all runs\")\n",
    "        \n",
    "        # Get the value column (usually 'value' or the third column)\n",
    "        value_col = None\n",
    "        for col in ['value', 'data', rtf_data.columns[2] if len(rtf_data.columns) > 2 else None]:\n",
    "            if col and col in rtf_data.columns:\n",
    "                value_col = col\n",
    "                break\n",
    "        \n",
    "        if value_col:\n",
    "            # Convert to numeric if needed\n",
    "            rtf_data[value_col] = pd.to_numeric(rtf_data[value_col], errors='coerce')\n",
    "            rtf_data = rtf_data.dropna(subset=[value_col])\n",
    "            \n",
    "            print(f\"Real-time factor statistics:\")\n",
    "            print(f\"  Overall mean: {rtf_data[value_col].mean():.4f}\")\n",
    "            print(f\"  Overall median: {rtf_data[value_col].median():.4f}\")\n",
    "            print(f\"  Overall std: {rtf_data[value_col].std():.4f}\")\n",
    "            print(f\"  Min: {rtf_data[value_col].min():.4f}\")\n",
    "            print(f\"  Max: {rtf_data[value_col].max():.4f}\")\n",
    "            \n",
    "            # Statistics per run\n",
    "            print(f\"\\nReal-time factor by run:\")\n",
    "            rtf_by_run = rtf_data.groupby('run_id')[value_col].agg(['count', 'mean', 'median', 'std', 'min', 'max'])\n",
    "            print(rtf_by_run)\n",
    "            \n",
    "        else:\n",
    "            print(\"Could not find value column for real_time_factor data\")\n",
    "            print(f\"Available columns: {list(rtf_data.columns)}\")\n",
    "    else:\n",
    "        print(\"No real_time_factor data found\")\n",
    "        if 'topic' in combined_df.columns:\n",
    "            unique_topics = combined_df['topic'].unique()\n",
    "            print(f\"Available topics: {list(unique_topics)}\")\n",
    "else:\n",
    "    print(\"No data available for real_time_factor analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ffcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Time Factor Visualizations\n",
    "if all_dataframes and len(combined_df) > 0:\n",
    "    rtf_data = combined_df[combined_df['topic'] == '/gazebo/real_time_factor'] if 'topic' in combined_df.columns else combined_df\n",
    "    \n",
    "    if len(rtf_data) > 0:\n",
    "        # Get the value column\n",
    "        value_col = None\n",
    "        for col in ['value', 'data', rtf_data.columns[2] if len(rtf_data.columns) > 2 else None]:\n",
    "            if col and col in rtf_data.columns:\n",
    "                value_col = col\n",
    "                break\n",
    "        \n",
    "        if value_col and 'timestamp' in rtf_data.columns:\n",
    "            rtf_data[value_col] = pd.to_numeric(rtf_data[value_col], errors='coerce')\n",
    "            rtf_data = rtf_data.dropna(subset=[value_col])\n",
    "            \n",
    "            # Create visualizations\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            \n",
    "            # 1. Time series plot for all runs\n",
    "            ax1 = axes[0, 0]\n",
    "            unique_runs = rtf_data['run_id'].unique()[:10]  # Limit to first 10 runs for clarity\n",
    "            colors = plt.cm.tab10(np.linspace(0, 1, len(unique_runs)))\n",
    "            \n",
    "            for i, run_id in enumerate(unique_runs):\n",
    "                run_rtf = rtf_data[rtf_data['run_id'] == run_id]\n",
    "                if len(run_rtf) > 0:\n",
    "                    ax1.plot(run_rtf['timestamp'], run_rtf[value_col], \n",
    "                            alpha=0.7, linewidth=1, color=colors[i], label=f'Run {i+1}')\n",
    "            \n",
    "            ax1.axhline(y=1.0, color='red', linestyle='--', alpha=0.8, label='Real-time (1.0)')\n",
    "            ax1.set_xlabel('Timestamp')\n",
    "            ax1.set_ylabel('Real Time Factor')\n",
    "            ax1.set_title('Real Time Factor Over Time (First 10 Runs)')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            \n",
    "            # 2. Distribution histogram\n",
    "            ax2 = axes[0, 1]\n",
    "            ax2.hist(rtf_data[value_col], bins=50, alpha=0.7, edgecolor='black')\n",
    "            ax2.axvline(x=1.0, color='red', linestyle='--', alpha=0.8, label='Real-time (1.0)')\n",
    "            ax2.axvline(x=rtf_data[value_col].mean(), color='green', linestyle='-', alpha=0.8, label=f'Mean ({rtf_data[value_col].mean():.3f})')\n",
    "            ax2.set_xlabel('Real Time Factor')\n",
    "            ax2.set_ylabel('Frequency')\n",
    "            ax2.set_title('Real Time Factor Distribution')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # 3. Box plot by run (first 15 runs)\n",
    "            ax3 = axes[1, 0]\n",
    "            rtf_by_run = []\n",
    "            run_labels = []\n",
    "            for i, run_id in enumerate(unique_runs[:15]):\n",
    "                run_rtf = rtf_data[rtf_data['run_id'] == run_id][value_col]\n",
    "                if len(run_rtf) > 0:\n",
    "                    rtf_by_run.append(run_rtf)\n",
    "                    run_labels.append(f'Run {i+1}')\n",
    "            \n",
    "            if rtf_by_run:\n",
    "                bp = ax3.boxplot(rtf_by_run, labels=run_labels, patch_artist=True)\n",
    "                for patch in bp['boxes']:\n",
    "                    patch.set_facecolor('lightblue')\n",
    "                    patch.set_alpha(0.7)\n",
    "                ax3.axhline(y=1.0, color='red', linestyle='--', alpha=0.8, label='Real-time (1.0)')\n",
    "                ax3.set_xlabel('Run')\n",
    "                ax3.set_ylabel('Real Time Factor')\n",
    "                ax3.set_title('Real Time Factor by Run (Box Plot)')\n",
    "                ax3.grid(True, alpha=0.3)\n",
    "                ax3.tick_params(axis='x', rotation=45)\n",
    "                ax3.legend()\n",
    "            \n",
    "            # 4. Performance metrics summary\n",
    "            ax4 = axes[1, 1]\n",
    "            ax4.axis('off')\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            rtf_stats = rtf_data.groupby('run_id')[value_col].agg(['mean', 'std', 'min', 'max']).round(4)\n",
    "            \n",
    "            performance_text = f\"Real Time Factor Summary ({len(rtf_data)} data points, {len(unique_runs)} runs):\\n\\n\"\n",
    "            performance_text += f\"Overall Statistics:\\n\"\n",
    "            performance_text += f\"  Mean: {rtf_data[value_col].mean():.4f}\\n\"\n",
    "            performance_text += f\"  Median: {rtf_data[value_col].median():.4f}\\n\"\n",
    "            performance_text += f\"  Std Dev: {rtf_data[value_col].std():.4f}\\n\"\n",
    "            performance_text += f\"  Min: {rtf_data[value_col].min():.4f}\\n\"\n",
    "            performance_text += f\"  Max: {rtf_data[value_col].max():.4f}\\n\\n\"\n",
    "            \n",
    "            # Performance categories\n",
    "            excellent = (rtf_data[value_col] >= 0.95).sum()\n",
    "            good = ((rtf_data[value_col] >= 0.8) & (rtf_data[value_col] < 0.95)).sum()\n",
    "            poor = (rtf_data[value_col] < 0.8).sum()\n",
    "            \n",
    "            performance_text += f\"Performance Distribution:\\n\"\n",
    "            performance_text += f\"  Excellent (≥0.95): {excellent} ({excellent/len(rtf_data)*100:.1f}%)\\n\"\n",
    "            performance_text += f\"  Good (0.8-0.95): {good} ({good/len(rtf_data)*100:.1f}%)\\n\"\n",
    "            performance_text += f\"  Poor (<0.8): {poor} ({poor/len(rtf_data)*100:.1f}%)\\n\\n\"\n",
    "            \n",
    "            performance_text += f\"Run-wise Averages (first 10):\\n\"\n",
    "            for i, (run_id, stats) in enumerate(rtf_stats.head(10).iterrows()):\n",
    "                performance_text += f\"  Run {i+1}: {stats['mean']:.4f} (±{stats['std']:.4f})\\n\"\n",
    "            \n",
    "            ax4.text(0.05, 0.95, performance_text, transform=ax4.transAxes, fontsize=10,\n",
    "                    verticalalignment='top', fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            print(f\"Missing required columns for visualization. Available: {list(rtf_data.columns)}\")\n",
    "    else:\n",
    "        print(\"No real_time_factor data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476669ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Real Time Factor Analysis with Better Run Identification\n",
    "if all_dataframes and len(combined_df) > 0:\n",
    "    rtf_data = combined_df[combined_df['topic'] == '/gazebo/real_time_factor'].copy() if 'topic' in combined_df.columns else combined_df.copy()\n",
    "    \n",
    "    if len(rtf_data) > 0:\n",
    "        # Get the value column\n",
    "        value_col = None\n",
    "        for col in ['value', 'data', rtf_data.columns[2] if len(rtf_data.columns) > 2 else None]:\n",
    "            if col and col in rtf_data.columns:\n",
    "                value_col = col\n",
    "                break\n",
    "        \n",
    "        if value_col and 'timestamp' in rtf_data.columns:\n",
    "            # Create better run identification using file paths\n",
    "            rtf_data['file_path'] = ''\n",
    "            for i, (df, csv_file) in enumerate(zip(all_dataframes, csv_file_paths)):\n",
    "                if 'topic' in df.columns:\n",
    "                    mask = df['topic'] == '/gazebo/real_time_factor'\n",
    "                else:\n",
    "                    mask = df.index >= 0  # all rows for non-topic data\n",
    "                \n",
    "                # Create a unique run identifier based on file name\n",
    "                file_name = Path(csv_file).stem\n",
    "                rtf_data.loc[rtf_data.index.isin(df[mask].index), 'file_path'] = file_name\n",
    "            \n",
    "            # Convert to numeric\n",
    "            rtf_data[value_col] = pd.to_numeric(rtf_data[value_col], errors='coerce')\n",
    "            rtf_data = rtf_data.dropna(subset=[value_col])\n",
    "            \n",
    "            # Analyze performance degradation over time\n",
    "            print(\"\\\\n\" + \"=\"*80)\n",
    "            print(\"DETAILED REAL TIME FACTOR ANALYSIS\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # Time-based analysis - look for performance degradation\n",
    "            rtf_data['timestamp_numeric'] = pd.to_numeric(rtf_data['timestamp'], errors='coerce')\n",
    "            rtf_data = rtf_data.dropna(subset=['timestamp_numeric'])\n",
    "            rtf_data = rtf_data.sort_values('timestamp_numeric')\n",
    "            \n",
    "            # Divide into time segments to analyze degradation\n",
    "            time_segments = 10\n",
    "            segment_size = len(rtf_data) // time_segments\n",
    "            \n",
    "            print(f\"\\nTime-based Performance Analysis (divided into {time_segments} segments):\")\n",
    "            print(\"-\" * 60)\n",
    "            for i in range(time_segments):\n",
    "                start_idx = i * segment_size\n",
    "                end_idx = (i + 1) * segment_size if i < time_segments - 1 else len(rtf_data)\n",
    "                segment_data = rtf_data.iloc[start_idx:end_idx]\n",
    "                \n",
    "                if len(segment_data) > 0:\n",
    "                    mean_rtf = segment_data[value_col].mean()\n",
    "                    min_rtf = segment_data[value_col].min()\n",
    "                    max_rtf = segment_data[value_col].max()\n",
    "                    std_rtf = segment_data[value_col].std()\n",
    "                    \n",
    "                    start_time = segment_data['timestamp_numeric'].min()\n",
    "                    end_time = segment_data['timestamp_numeric'].max()\n",
    "                    \n",
    "                    print(f\"Segment {i+1:2d} (t={start_time:6.1f}-{end_time:6.1f}s): \"\n",
    "                          f\"Mean={mean_rtf:.4f}, Min={min_rtf:.4f}, Max={max_rtf:.4f}, Std={std_rtf:.4f}\")\n",
    "            \n",
    "            # Identify problematic periods\n",
    "            poor_performance = rtf_data[rtf_data[value_col] < 0.5]\n",
    "            if len(poor_performance) > 0:\n",
    "                print(f\"\\nPoor Performance Periods (RTF < 0.5): {len(poor_performance)} data points\")\n",
    "                print(f\"Worst RTF: {poor_performance[value_col].min():.4f} at timestamp {poor_performance.loc[poor_performance[value_col].idxmin(), 'timestamp_numeric']:.1f}s\")\n",
    "            \n",
    "            # Performance stability analysis\n",
    "            rolling_mean = rtf_data[value_col].rolling(window=100, min_periods=1).mean()\n",
    "            rolling_std = rtf_data[value_col].rolling(window=100, min_periods=1).std()\n",
    "            \n",
    "            print(f\"\\nPerformance Stability Analysis:\")\n",
    "            print(f\"Average rolling standard deviation (100-point window): {rolling_std.mean():.4f}\")\n",
    "            print(f\"Performance coefficient of variation: {rtf_data[value_col].std() / rtf_data[value_col].mean():.4f}\")\n",
    "            \n",
    "            # System performance recommendations\n",
    "            print(f\"\\nSystem Performance Assessment:\")\n",
    "            excellent_pct = (rtf_data[value_col] >= 0.95).mean() * 100\n",
    "            good_pct = ((rtf_data[value_col] >= 0.8) & (rtf_data[value_col] < 0.95)).mean() * 100\n",
    "            fair_pct = ((rtf_data[value_col] >= 0.5) & (rtf_data[value_col] < 0.8)).mean() * 100\n",
    "            poor_pct = (rtf_data[value_col] < 0.5).mean() * 100\n",
    "            \n",
    "            print(f\"  Excellent (≥0.95): {excellent_pct:.1f}%\")\n",
    "            print(f\"  Good (0.8-0.94):   {good_pct:.1f}%\") \n",
    "            print(f\"  Fair (0.5-0.79):   {fair_pct:.1f}%\")\n",
    "            print(f\"  Poor (<0.5):       {poor_pct:.1f}%\")\n",
    "            \n",
    "            if excellent_pct > 70:\n",
    "                print(\"\\n✅ ASSESSMENT: System performance is EXCELLENT\")\n",
    "            elif excellent_pct + good_pct > 80:\n",
    "                print(\"\\n✅ ASSESSMENT: System performance is GOOD\")\n",
    "            elif poor_pct < 10:\n",
    "                print(\"\\n⚠️  ASSESSMENT: System performance is ACCEPTABLE but could be improved\")\n",
    "            else:\n",
    "                print(\"\\\\n❌ ASSESSMENT: System performance has SIGNIFICANT ISSUES\")\n",
    "                print(\"   Recommendations:\")\n",
    "                print(\"   - Consider hardware upgrades (CPU, RAM)\")\n",
    "                print(\"   - Optimize simulation parameters\")\n",
    "                print(\"   - Reduce simulation complexity\")\n",
    "                print(\"   - Check for resource bottlenecks\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"Missing required columns for enhanced analysis. Available: {list(rtf_data.columns)}\")\n",
    "    else:\n",
    "        print(\"No real_time_factor data available for enhanced analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Time Factor Comparison Across Run Types\n",
    "if all_dataframes and len(combined_df) > 0:\n",
    "    rtf_data = combined_df[combined_df['topic'] == '/gazebo/real_time_factor'].copy() if 'topic' in combined_df.columns else combined_df.copy()\n",
    "    \n",
    "    if len(rtf_data) > 0:\n",
    "        value_col = None\n",
    "        for col in ['value', 'data', rtf_data.columns[2] if len(rtf_data.columns) > 2 else None]:\n",
    "            if col and col in rtf_data.columns:\n",
    "                value_col = col\n",
    "                break\n",
    "        \n",
    "        if value_col:\n",
    "            rtf_data[value_col] = pd.to_numeric(rtf_data[value_col], errors='coerce')\n",
    "            rtf_data = rtf_data.dropna(subset=[value_col])\n",
    "            \n",
    "            # Extract run characteristics from file paths\n",
    "            run_characteristics = []\n",
    "            for csv_file in csv_file_paths:\n",
    "                file_name = Path(csv_file).stem\n",
    "                \n",
    "                # Extract characteristics from filename\n",
    "                characteristics = {\n",
    "                    'file': file_name,\n",
    "                    'scenario_type': 'unknown',\n",
    "                    'parameters': 'unknown'\n",
    "                }\n",
    "                \n",
    "                # Try to extract scenario type and parameters\n",
    "                if 'rooms' in file_name.lower():\n",
    "                    characteristics['scenario_type'] = 'rooms'\n",
    "                elif 'circle' in file_name.lower():\n",
    "                    characteristics['scenario_type'] = 'circle'\n",
    "                elif 'hallway' in file_name.lower():\n",
    "                    characteristics['scenario_type'] = 'hallway'\n",
    "                elif 'large-room' in file_name.lower():\n",
    "                    characteristics['scenario_type'] = 'large-room'\n",
    "                \n",
    "                # Extract parameter information\n",
    "                if '-p1-' in file_name:\n",
    "                    characteristics['parameters'] = 'p1'\n",
    "                elif '-p2-' in file_name:\n",
    "                    characteristics['parameters'] = 'p2'\n",
    "                elif '-p3-' in file_name:\n",
    "                    characteristics['parameters'] = 'p3'\n",
    "                \n",
    "                run_characteristics.append(characteristics)\n",
    "            \n",
    "            # Create final summary visualization\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "            \n",
    "            # 1. Overall performance timeline with rolling average\n",
    "            ax1 = axes[0]\n",
    "            if 'timestamp' in rtf_data.columns:\n",
    "                rtf_data['timestamp_numeric'] = pd.to_numeric(rtf_data['timestamp'], errors='coerce')\n",
    "                rtf_data = rtf_data.sort_values('timestamp_numeric')\n",
    "                \n",
    "                # Plot raw data\n",
    "                ax1.scatter(rtf_data['timestamp_numeric'], rtf_data[value_col], \n",
    "                           alpha=0.1, s=1, color='lightblue', label='Individual measurements')\n",
    "                \n",
    "                # Plot rolling average\n",
    "                window_size = max(100, len(rtf_data) // 100)\n",
    "                rolling_mean = rtf_data[value_col].rolling(window=window_size, min_periods=1).mean()\n",
    "                ax1.plot(rtf_data['timestamp_numeric'], rolling_mean, \n",
    "                        color='darkblue', linewidth=2, label=f'Rolling average ({window_size} pts)')\n",
    "                \n",
    "                ax1.axhline(y=1.0, color='red', linestyle='--', alpha=0.8, label='Real-time (1.0)')\n",
    "                ax1.axhline(y=0.8, color='orange', linestyle=':', alpha=0.8, label='Acceptable (0.8)')\n",
    "                \n",
    "                ax1.set_xlabel('Timestamp (seconds)')\n",
    "                ax1.set_ylabel('Real Time Factor')\n",
    "                ax1.set_title('Real Time Factor Performance Timeline')\n",
    "                ax1.legend()\n",
    "                ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # 2. Performance distribution with statistics\n",
    "            ax2 = axes[1]\n",
    "            n, bins, patches = ax2.hist(rtf_data[value_col], bins=50, alpha=0.7, \n",
    "                                       edgecolor='black', color='skyblue')\n",
    "            \n",
    "            # Color code the histogram bars\n",
    "            for i, patch in enumerate(patches):\n",
    "                bin_center = (bins[i] + bins[i+1]) / 2\n",
    "                if bin_center >= 0.95:\n",
    "                    patch.set_facecolor('green')\n",
    "                    patch.set_alpha(0.8)\n",
    "                elif bin_center >= 0.8:\n",
    "                    patch.set_facecolor('orange')\n",
    "                    patch.set_alpha(0.8)\n",
    "                elif bin_center < 0.5:\n",
    "                    patch.set_facecolor('red')\n",
    "                    patch.set_alpha(0.8)\n",
    "            \n",
    "            ax2.axvline(x=1.0, color='red', linestyle='--', alpha=0.8, linewidth=2, label='Real-time (1.0)')\n",
    "            ax2.axvline(x=rtf_data[value_col].mean(), color='purple', linestyle='-', \n",
    "                       alpha=0.8, linewidth=2, label=f'Mean ({rtf_data[value_col].mean():.3f})')\n",
    "            ax2.axvline(x=rtf_data[value_col].median(), color='brown', linestyle='-', \n",
    "                       alpha=0.8, linewidth=2, label=f'Median ({rtf_data[value_col].median():.3f})')\n",
    "            \n",
    "            ax2.set_xlabel('Real Time Factor')\n",
    "            ax2.set_ylabel('Frequency')\n",
    "            ax2.set_title('Real Time Factor Distribution')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # 3. Summary statistics table\n",
    "            ax3 = axes[2]\n",
    "            ax3.axis('off')\n",
    "            \n",
    "            # Calculate comprehensive statistics\n",
    "            stats_text = \"REAL TIME FACTOR SUMMARY\\n\"\n",
    "            stats_text += \"=\" * 30 + \"\\n\\n\"\n",
    "            \n",
    "            stats_text += f\"Dataset Overview:\\n\"\n",
    "            stats_text += f\"  Total measurements: {len(rtf_data):,}\\n\"\n",
    "            stats_text += f\"  Total runs: {len(csv_file_paths)}\\n\"\n",
    "            stats_text += f\"  Time span: {rtf_data['timestamp_numeric'].max() - rtf_data['timestamp_numeric'].min():.1f} seconds\\n\\n\"\n",
    "            \n",
    "            stats_text += f\"Performance Metrics:\\n\"\n",
    "            stats_text += f\"  Mean RTF: {rtf_data[value_col].mean():.4f}\\n\"\n",
    "            stats_text += f\"  Median RTF: {rtf_data[value_col].median():.4f}\\n\"\n",
    "            stats_text += f\"  Std Deviation: {rtf_data[value_col].std():.4f}\\n\"\n",
    "            stats_text += f\"  Min RTF: {rtf_data[value_col].min():.4f}\\n\"\n",
    "            stats_text += f\"  Max RTF: {rtf_data[value_col].max():.4f}\\n\\n\"\n",
    "            \n",
    "            # Percentiles\n",
    "            stats_text += f\"Performance Percentiles:\\n\"\n",
    "            for p in [10, 25, 50, 75, 90, 95, 99]:\n",
    "                val = np.percentile(rtf_data[value_col], p)\n",
    "                stats_text += f\"  {p:2d}th percentile: {val:.4f}\\n\"\n",
    "            \n",
    "            stats_text += f\"\\nPerformance Categories:\\n\"\n",
    "            excellent = (rtf_data[value_col] >= 0.95).sum()\n",
    "            good = ((rtf_data[value_col] >= 0.8) & (rtf_data[value_col] < 0.95)).sum()\n",
    "            fair = ((rtf_data[value_col] >= 0.5) & (rtf_data[value_col] < 0.8)).sum()\n",
    "            poor = (rtf_data[value_col] < 0.5).sum()\n",
    "            \n",
    "            stats_text += f\" Excellent (≥0.95): {excellent:,} ({excellent/len(rtf_data)*100:.1f}%)\\n\"\n",
    "            stats_text += f\" Good (0.8-0.95): {good:,} ({good/len(rtf_data)*100:.1f}%)\\n\"\n",
    "            stats_text += f\" Fair (0.5-0.8): {fair:,} ({fair/len(rtf_data)*100:.1f}%)\\n\"\n",
    "            stats_text += f\" Poor (<0.5): {poor:,} ({poor/len(rtf_data)*100:.1f}%)\\n\"\n",
    "            \n",
    "            # Final assessment\n",
    "            if excellent/len(rtf_data) > 0.7:\n",
    "                stats_text += f\"\\\\n Overall: EXCELLENT performance\\n\"\n",
    "            elif (excellent + good)/len(rtf_data) > 0.8:\n",
    "                stats_text += f\"\\\\n Overall: GOOD performance\\n\"  \n",
    "            elif poor/len(rtf_data) < 0.1:\n",
    "                stats_text += f\"\\\\n Overall: ACCEPTABLE performance\\n\"\n",
    "            else:\n",
    "                stats_text += f\"\\\\n Overall: POOR performance\\n\"\n",
    "            \n",
    "            ax3.text(0.05, 0.95, stats_text, transform=ax3.transAxes, fontsize=10,\n",
    "                    verticalalignment='top', fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.9))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\n🎯 REAL TIME FACTOR ANALYSIS COMPLETE\")\n",
    "            print(f\"   Analyzed {len(rtf_data):,} measurements from {len(csv_file_paths)} runs\")\n",
    "            print(f\"   Overall system performance rated as: {'EXCELLENT' if excellent/len(rtf_data) > 0.7 else 'GOOD' if (excellent + good)/len(rtf_data) > 0.8 else 'ACCEPTABLE' if poor/len(rtf_data) < 0.1 else 'POOR'}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Could not find value column for comparison analysis\")\n",
    "    else:\n",
    "        print(\"No real_time_factor data available for comparison analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301bb9dc",
   "metadata": {},
   "source": [
    "# Movement Duration Analysis\n",
    "\n",
    "This section analyzes the duration of movement for each run by calculating the derivative of position (linear speed) and yaw (angular speed). Movement is detected when either linear speed (from x/y position derivatives) or angular speed (from yaw derivative) exceeds 0.1 units. The movement duration is calculated from the first time movement is detected until the last time it stays below the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movement Duration Analysis - Calculate movement periods for each run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def calculate_movement_duration(df, speed_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Calculate movement duration based on linear and angular velocity\n",
    "    Returns start_time, end_time, duration in seconds\n",
    "    \"\"\"\n",
    "    if len(df) < 2:\n",
    "        return None, None, 0.0\n",
    "    \n",
    "    # Sort by timestamp to ensure proper derivative calculation\n",
    "    df_sorted = df.sort_values('timestamp').copy()\n",
    "    \n",
    "    # Convert timestamp to numeric if needed\n",
    "    df_sorted['timestamp_numeric'] = pd.to_numeric(df_sorted['timestamp'], errors='coerce')\n",
    "    df_sorted = df_sorted.dropna(subset=['timestamp_numeric'])\n",
    "    \n",
    "    if len(df_sorted) < 2:\n",
    "        return None, None, 0.0\n",
    "    \n",
    "    # Look for position data (x, y coordinates and yaw/orientation)\n",
    "    pos_cols = [col for col in df_sorted.columns if any(pattern in col.lower() for pattern in ['pose', 'position', 'x', 'y', 'yaw', 'orientation', 'theta'])]\n",
    "    \n",
    "    if not pos_cols:\n",
    "        print(f\"No position columns found in data. Available columns: {list(df_sorted.columns)}\")\n",
    "        return None, None, 0.0\n",
    "    \n",
    "    # Try different approaches to find movement data\n",
    "    movement_detected = pd.Series([False] * len(df_sorted), index=df_sorted.index)\n",
    "    \n",
    "    # Method 1: Look for pose/position topics\n",
    "    pose_topics = ['/amcl_pose', '/robot_pose', '/base_link', '/odom', '/pose']\n",
    "    for topic in pose_topics:\n",
    "        if 'topic' in df_sorted.columns:\n",
    "            topic_data = df_sorted[df_sorted['topic'] == topic].copy()\n",
    "            if len(topic_data) > 1:\n",
    "                # print(f\"Found {len(topic_data)} data points for topic: {topic}\")\n",
    "                movement_detected |= detect_movement_from_pose_data(topic_data, speed_threshold)\n",
    "    \n",
    "    # Method 2: Look for velocity topics directly\n",
    "    vel_topics = ['/cmd_vel', '/base_controller/cmd_vel', '/mobile_base_controller/cmd_vel', '/velocity']\n",
    "    for topic in vel_topics:\n",
    "        if 'topic' in df_sorted.columns:\n",
    "            topic_data = df_sorted[df_sorted['topic'] == topic].copy()\n",
    "            if len(topic_data) > 1:\n",
    "                # print(f\"Found {len(topic_data)} velocity data points for topic: {topic}\")\n",
    "                movement_detected |= detect_movement_from_velocity_data(topic_data, speed_threshold)\n",
    "    \n",
    "    # Method 3: If we have numeric columns that might be coordinates\n",
    "    if not movement_detected.any():\n",
    "        numeric_cols = df_sorted.select_dtypes(include=[np.number]).columns\n",
    "        potential_pos_cols = [col for col in numeric_cols if col not in ['timestamp', 'timestamp_numeric']]\n",
    "        \n",
    "        if len(potential_pos_cols) >= 2:  # At least x, y\n",
    "            # print(f\"Trying to detect movement from numeric columns: {potential_pos_cols[:3]}\")\n",
    "            movement_detected = detect_movement_from_numeric_cols(df_sorted, potential_pos_cols, speed_threshold)\n",
    "    \n",
    "    if not movement_detected.any():\n",
    "        # print(\"No movement detected in data\")\n",
    "        return None, None, 0.0\n",
    "    \n",
    "    # Find first and last movement\n",
    "    moving_indices = df_sorted.index[movement_detected]\n",
    "    if len(moving_indices) == 0:\n",
    "        return None, None, 0.0\n",
    "    \n",
    "    start_idx = moving_indices[0]\n",
    "    end_idx = moving_indices[-1]\n",
    "    \n",
    "    start_time = df_sorted.loc[start_idx, 'timestamp_numeric']\n",
    "    end_time = df_sorted.loc[end_idx, 'timestamp_numeric']\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    return start_time, end_time, duration\n",
    "\n",
    "def detect_movement_from_pose_data(pose_data, threshold):\n",
    "    \"\"\"Detect movement from pose/position data\"\"\"\n",
    "    movement = pd.Series([False] * len(pose_data), index=pose_data.index)\n",
    "    \n",
    "    if 'value' in pose_data.columns:\n",
    "        # Try to parse pose data from value column\n",
    "        try:\n",
    "            # This might contain position/orientation data\n",
    "            pose_data['value_numeric'] = pd.to_numeric(pose_data['value'], errors='coerce')\n",
    "            if pose_data['value_numeric'].notna().any():\n",
    "                vel = np.abs(np.diff(pose_data['value_numeric'].fillna(0)))\n",
    "                vel = np.append(vel, vel[-1] if len(vel) > 0 else 0)\n",
    "                movement = vel > threshold\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return pd.Series(movement, index=pose_data.index)\n",
    "\n",
    "def detect_movement_from_velocity_data(vel_data, threshold):\n",
    "    \"\"\"Detect movement from velocity data\"\"\"\n",
    "    movement = pd.Series([False] * len(vel_data), index=vel_data.index)\n",
    "    \n",
    "    if 'value' in vel_data.columns:\n",
    "        try:\n",
    "            vel_data['value_numeric'] = pd.to_numeric(vel_data['value'], errors='coerce')\n",
    "            movement = np.abs(vel_data['value_numeric'].fillna(0)) > threshold\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return pd.Series(movement, index=vel_data.index)\n",
    "\n",
    "def detect_movement_from_numeric_cols(df, cols, threshold):\n",
    "    \"\"\"Detect movement from numeric columns by calculating derivatives\"\"\"\n",
    "    movement = pd.Series([False] * len(df), index=df.index)\n",
    "    \n",
    "    for col in cols[:3]:  # Check up to 3 columns\n",
    "        try:\n",
    "            values = df[col].fillna(0)\n",
    "            if values.std() > 0:  # Only if there's variation\n",
    "                # Calculate derivative (velocity)\n",
    "                dt = np.diff(df['timestamp_numeric'])\n",
    "                dt = np.where(dt == 0, 1e-6, dt)  # Avoid division by zero\n",
    "                vel = np.abs(np.diff(values)) / dt\n",
    "                vel = np.append(vel, vel[-1] if len(vel) > 0 else 0)\n",
    "                movement |= vel > threshold\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column {col}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return movement\n",
    "\n",
    "# Analyze movement duration for all runs\n",
    "print(\"Analyzing movement duration for all runs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "movement_results = []\n",
    "\n",
    "for i, (df, csv_file) in enumerate(zip(all_dataframes, csv_file_paths)):\n",
    "    file_name = Path(csv_file).stem\n",
    "    # print(f\"\\nAnalyzing run {i+1}/{len(all_dataframes)}: {file_name}\")\n",
    "    \n",
    "    start_time, end_time, duration = calculate_movement_duration(df)\n",
    "    \n",
    "    movement_results.append({\n",
    "        'run_id': i + 1,\n",
    "        'file_name': file_name,\n",
    "        'csv_path': csv_file,\n",
    "        'start_time': start_time,\n",
    "        'end_time': end_time,\n",
    "        'duration_seconds': duration,\n",
    "        'duration_minutes': duration / 60.0 if duration > 0 else 0.0,\n",
    "        'total_data_points': len(df),\n",
    "        'has_movement': duration > 0\n",
    "    })\n",
    "    \n",
    "    if duration > 0:\n",
    "        pass\n",
    "        # print(f\"  ✅ Movement detected: {duration:.2f} seconds ({duration/60:.2f} minutes)\")\n",
    "        # print(f\"     Start: {start_time:.1f}s, End: {end_time:.1f}s\")\n",
    "    else:\n",
    "        print(f\"  ❌ No movement detected\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "movement_df = pd.DataFrame(movement_results)\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"MOVEMENT DURATION SUMMARY\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Total runs analyzed: {len(movement_results)}\")\n",
    "print(f\"Runs with detected movement: {movement_df['has_movement'].sum()}\")\n",
    "print(f\"Runs without detected movement: {(~movement_df['has_movement']).sum()}\")\n",
    "\n",
    "if movement_df['has_movement'].any():\n",
    "    moving_runs = movement_df[movement_df['has_movement']]\n",
    "    print(f\"\\nMovement Statistics:\")\n",
    "    print(f\"  Average duration: {moving_runs['duration_seconds'].mean():.2f} seconds ({moving_runs['duration_minutes'].mean():.2f} minutes)\")\n",
    "    print(f\"  Median duration: {moving_runs['duration_seconds'].median():.2f} seconds ({moving_runs['duration_minutes'].median():.2f} minutes)\")\n",
    "    print(f\"  Min duration: {moving_runs['duration_seconds'].min():.2f} seconds\")\n",
    "    print(f\"  Max duration: {moving_runs['duration_seconds'].max():.2f} seconds\")\n",
    "    print(f\"  Std deviation: {moving_runs['duration_seconds'].std():.2f} seconds\")\n",
    "\n",
    "movement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e40dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movement Duration Visualizations\n",
    "if len(movement_df) > 0:\n",
    "    moving_runs = movement_df[movement_df['has_movement']]\n",
    "    \n",
    "    if len(moving_runs) > 0:\n",
    "        # Create comprehensive visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Bar chart of movement durations\n",
    "        ax1 = axes[0, 0]\n",
    "        run_indices = range(len(moving_runs))\n",
    "        bars = ax1.bar(run_indices, moving_runs['duration_minutes'], \n",
    "                      color='skyblue', alpha=0.7, edgecolor='navy')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (idx, row) in enumerate(moving_runs.iterrows()):\n",
    "            ax1.text(i, row['duration_minutes'] + 0.1, f\"{row['duration_minutes']:.1f}m\",\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        ax1.set_xlabel('Run Number')\n",
    "        ax1.set_ylabel('Movement Duration (minutes)')\n",
    "        ax1.set_title(f'Movement Duration by Run ({len(moving_runs)} runs with movement)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Set x-tick labels to show run IDs\n",
    "        ax1.set_xticks(run_indices[::max(1, len(run_indices)//10)])  # Show every nth tick\n",
    "        ax1.set_xticklabels([f\"R{moving_runs.iloc[i]['run_id']}\" for i in run_indices[::max(1, len(run_indices)//10)]], rotation=45)\n",
    "        \n",
    "        # 2. Histogram of movement durations\n",
    "        ax2 = axes[0, 1]\n",
    "        n_bins = min(20, len(moving_runs) // 2 + 1)\n",
    "        n, bins, patches = ax2.hist(moving_runs['duration_minutes'], bins=n_bins, \n",
    "                                   alpha=0.7, color='lightgreen', edgecolor='darkgreen')\n",
    "        \n",
    "        # Color code histogram bars\n",
    "        mean_duration = moving_runs['duration_minutes'].mean()\n",
    "        for i, patch in enumerate(patches):\n",
    "            bin_center = (bins[i] + bins[i+1]) / 2\n",
    "            if bin_center > mean_duration * 1.2:\n",
    "                patch.set_facecolor('orange')\n",
    "            elif bin_center < mean_duration * 0.8:\n",
    "                patch.set_facecolor('lightblue')\n",
    "        \n",
    "        ax2.axvline(x=mean_duration, color='red', linestyle='--', \n",
    "                   label=f'Mean: {mean_duration:.1f} min')\n",
    "        ax2.axvline(x=moving_runs['duration_minutes'].median(), color='purple', linestyle=':', \n",
    "                   label=f'Median: {moving_runs[\"duration_minutes\"].median():.1f} min')\n",
    "        \n",
    "        ax2.set_xlabel('Movement Duration (minutes)')\n",
    "        ax2.set_ylabel('Number of Runs')\n",
    "        ax2.set_title('Distribution of Movement Durations')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Movement success rate and statistics\n",
    "        ax3 = axes[1, 0]\n",
    "        success_rate = movement_df['has_movement'].mean() * 100\n",
    "        \n",
    "        # Pie chart of movement detection\n",
    "        labels = ['Movement Detected', 'No Movement']\n",
    "        sizes = [movement_df['has_movement'].sum(), (~movement_df['has_movement']).sum()]\n",
    "        colors = ['lightgreen', 'lightcoral']\n",
    "        explode = (0.05, 0)  # explode the first slice\n",
    "        \n",
    "        wedges, texts, autotexts = ax3.pie(sizes, labels=labels, colors=colors, explode=explode,\n",
    "                                          autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "        ax3.set_title(f'Movement Detection Success Rate\\\\n({len(movement_df)} total runs)')\n",
    "        \n",
    "        # 4. Detailed statistics table\n",
    "        ax4 = axes[1, 1]\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        stats_text = \"MOVEMENT DURATION ANALYSIS\\n\"\n",
    "        stats_text += \"=\" * 35 + \"\\n\\n\"\n",
    "        \n",
    "        stats_text += f\"Dataset Overview:\\n\"\n",
    "        stats_text += f\"  Total runs: {len(movement_df)}\\n\"\n",
    "        stats_text += f\"  Runs with movement: {len(moving_runs)} ({success_rate:.1f}%)\\n\"\n",
    "        stats_text += f\"  Runs without movement: {len(movement_df) - len(moving_runs)}\\n\\n\"\n",
    "        \n",
    "        if len(moving_runs) > 0:\n",
    "            stats_text += f\"Duration Statistics (moving runs only):\\n\"\n",
    "            stats_text += f\"  Mean: {moving_runs['duration_minutes'].mean():.2f} minutes\\n\"\n",
    "            stats_text += f\"  Median: {moving_runs['duration_minutes'].median():.2f} minutes\\n\"\n",
    "            stats_text += f\"  Std Dev: {moving_runs['duration_minutes'].std():.2f} minutes\\n\"\n",
    "            stats_text += f\"  Min: {moving_runs['duration_minutes'].min():.2f} minutes\\n\"\n",
    "            stats_text += f\"  Max: {moving_runs['duration_minutes'].max():.2f} minutes\\n\\n\"\n",
    "            \n",
    "            stats_text += f\"Duration Categories:\\n\"\n",
    "            short_runs = (moving_runs['duration_minutes'] < 2).sum()\n",
    "            medium_runs = ((moving_runs['duration_minutes'] >= 2) & \n",
    "                          (moving_runs['duration_minutes'] < 5)).sum()\n",
    "            long_runs = (moving_runs['duration_minutes'] >= 5).sum()\n",
    "            \n",
    "            stats_text += f\"  Short (<2 min): {short_runs} runs\\n\"\n",
    "            stats_text += f\"  Medium (2-5 min): {medium_runs} runs\\n\"\n",
    "            stats_text += f\"  Long (≥5 min): {long_runs} runs\\n\\n\"\n",
    "            \n",
    "            # Find interesting cases\n",
    "            if len(moving_runs) > 1:\n",
    "                shortest_run = moving_runs.loc[moving_runs['duration_minutes'].idxmin()]\n",
    "                longest_run = moving_runs.loc[moving_runs['duration_minutes'].idxmax()]\n",
    "                \n",
    "                stats_text += f\"Extreme Cases:\\n\"\n",
    "                stats_text += f\"  Shortest: Run {shortest_run['run_id']} ({shortest_run['duration_minutes']:.2f} min)\\n\"\n",
    "                stats_text += f\"  Longest: Run {longest_run['run_id']} ({longest_run['duration_minutes']:.2f} min)\\n\"\n",
    "        \n",
    "        ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=11,\n",
    "                verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.9))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed run-by-run results\n",
    "        print(f\"\\n📊 DETAILED MOVEMENT DURATION RESULTS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if len(moving_runs) > 0:\n",
    "            print(\"\\nRuns with detected movement:\")\n",
    "            print(\"-\" * 70)\n",
    "            for _, run in moving_runs.iterrows():\n",
    "                print(f\"Run {run['run_id']:3d}: {run['duration_minutes']:6.2f} min ({run['duration_seconds']:7.1f}s) - {run['file_name']}\")\n",
    "        \n",
    "        no_movement_runs = movement_df[~movement_df['has_movement']]\n",
    "        if len(no_movement_runs) > 0:\n",
    "            print(\"\\nRuns without detected movement:\")\n",
    "            print(\"-\" * 70)\n",
    "            for _, run in no_movement_runs.iterrows():\n",
    "                print(f\"Run {run['run_id']:3d}: No movement detected - {run['file_name']}\")\n",
    "        \n",
    "        print(f\"\\n✅ Analysis complete! {success_rate:.1f}% of runs had detectable movement.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No runs with detectable movement found.\")\n",
    "        print(\"\\nPossible reasons:\")\n",
    "        print(\"- No position/velocity data in the CSV files\")\n",
    "        print(\"- Movement speed below threshold (0.1 units/s)\")\n",
    "        print(\"- Data format not recognized\")\n",
    "        print(\"\\nTry checking the data structure and topics available.\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No movement analysis data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed comparison table and export results\n",
    "if len(movement_df) > 0 and movement_df['has_movement'].any():\n",
    "    # Create a detailed comparison table\n",
    "    print(\"📋 MOVEMENT DURATION COMPARISON TABLE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Sort by duration for better comparison\n",
    "    comparison_df = movement_df[movement_df['has_movement']].copy()\n",
    "    comparison_df = comparison_df.sort_values('duration_seconds')\n",
    "    \n",
    "    # Add ranking and performance categories\n",
    "    comparison_df['rank'] = range(1, len(comparison_df) + 1)\n",
    "    comparison_df['performance_category'] = comparison_df['duration_minutes'].apply(\n",
    "        lambda x: 'Short (< 0.5 min)' if x < 0.5 \n",
    "        else 'Quick (0.5-0.7 min)' if x < 0.7\n",
    "        else 'Medium (0.7-1.0 min)' if x < 1.0\n",
    "        else 'Long (≥ 1.0 min)'\n",
    "    )\n",
    "    \n",
    "    # Create formatted display table\n",
    "    display_cols = ['rank', 'run_id', 'file_name', 'duration_minutes', 'duration_seconds', 'performance_category']\n",
    "    display_df = comparison_df[display_cols].copy()\n",
    "    display_df.columns = ['Rank', 'Run ID', 'File Name', 'Duration (min)', 'Duration (sec)', 'Category']\n",
    "    \n",
    "    # Round durations for better display\n",
    "    display_df['Duration (min)'] = display_df['Duration (min)'].round(2)\n",
    "    display_df['Duration (sec)'] = display_df['Duration (sec)'].round(1)\n",
    "    \n",
    "    print(\"\\nMovement Duration Rankings (shortest to longest):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Print table with formatting\n",
    "    header = f\"{'Rank':>4} {'Run':>4} {'Duration':>12} {'Duration':>12} {'Category':>20} {'File Name':>15}\"\n",
    "    print(header)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for _, row in display_df.iterrows():\n",
    "        print(f\"{row['Rank']:4d} {row['Run ID']:4d} {row['Duration (min)']:8.2f} min {row['Duration (sec)']:8.1f} sec \"\n",
    "              f\"{row['Category']:>20} {row['File Name'][:15]:>15}\")\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(f\"\\n📈 STATISTICAL SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    durations = comparison_df['duration_seconds']\n",
    "    print(f\"Number of runs with movement: {len(comparison_df)}\")\n",
    "    print(f\"Average duration: {durations.mean():.2f} ± {durations.std():.2f} seconds\")\n",
    "    print(f\"Median duration: {durations.median():.2f} seconds\")\n",
    "    print(f\"Range: {durations.min():.1f} - {durations.max():.1f} seconds\")\n",
    "    print(f\"Duration span: {durations.max() - durations.min():.1f} seconds\")\n",
    "    \n",
    "    # Category breakdown\n",
    "    print(f\"\\n🏷️  CATEGORY BREAKDOWN\")\n",
    "    print(\"-\" * 30)\n",
    "    category_counts = comparison_df['performance_category'].value_counts()\n",
    "    for category, count in category_counts.items():\n",
    "        percentage = (count / len(comparison_df)) * 100\n",
    "        print(f\"{category:>20}: {count:2d} runs ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Identify potential outliers\n",
    "    q1 = durations.quantile(0.25)\n",
    "    q3 = durations.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    outliers = comparison_df[(durations < lower_bound) | (durations > upper_bound)]\n",
    "    \n",
    "    if len(outliers) > 0:\n",
    "        print(f\"\\n⚠️  POTENTIAL OUTLIERS ({len(outliers)} runs)\")\n",
    "        print(\"-\" * 40)\n",
    "        for _, outlier in outliers.iterrows():\n",
    "            outlier_type = \"Short\" if outlier['duration_seconds'] < lower_bound else \"Long\"\n",
    "            print(f\"Run {outlier['run_id']:3d}: {outlier['duration_seconds']:6.1f}s ({outlier_type}) - {outlier['file_name']}\")\n",
    "    else:\n",
    "        print(f\"\\n✅ No significant outliers detected\")\n",
    "    \n",
    "    # Export results to CSV if needed\n",
    "    export_df = comparison_df[['run_id', 'file_name', 'start_time', 'end_time', \n",
    "                              'duration_seconds', 'duration_minutes', 'performance_category']].copy()\n",
    "    \n",
    "    print(f\"\\n💾 Results summary prepared for export ({len(export_df)} runs)\")\n",
    "    print(\"   Available data: run_id, file_name, start_time, end_time, duration_seconds, duration_minutes, category\")\n",
    "    \n",
    "    # Show final summary\n",
    "    most_consistent = comparison_df.iloc[len(comparison_df)//2] if len(comparison_df) > 1 else comparison_df.iloc[0]\n",
    "    fastest_run = comparison_df.iloc[0]\n",
    "    slowest_run = comparison_df.iloc[-1]\n",
    "    \n",
    "    print(f\"\\n🎯 KEY FINDINGS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Fastest run: Run {fastest_run['run_id']} ({fastest_run['duration_seconds']:.1f}s)\")\n",
    "    print(f\"Slowest run: Run {slowest_run['run_id']} ({slowest_run['duration_seconds']:.1f}s)\")\n",
    "    print(f\"Most typical: Run {most_consistent['run_id']} ({most_consistent['duration_seconds']:.1f}s)\")\n",
    "    print(f\"Time variation: {((slowest_run['duration_seconds'] - fastest_run['duration_seconds']) / fastest_run['duration_seconds'] * 100):.1f}% difference between fastest and slowest\")\n",
    "    \n",
    "    # Store results in variable for potential further analysis\n",
    "    movement_comparison_results = export_df\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No movement data available for detailed comparison\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
